# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HvKwa8w6gIUa1jYtdARiE2o6Vuy7ZR_g
"""

##Read dataset
import pandas as pd
d=pd.read_csv('trojantrojannew.csv')
d

"""# New Section"""

dummies=pd.get_dummies(d.Label)
dummies

d=pd.concat([d,dummies],axis='columns')
d=d.drop(["Label","Trojan Free"],axis='columns')
X=d.drop(["Trojan Infected"],axis='columns')
d.rename(index=str,columns={"Trojan Infected":"Trojan"})
Y=d[("Trojan Infected")]
X=X.drop(['Circuits'],axis='columns')
# d=pd.concat([d,dummies],axis='columns')
# d=d.drop(["happy",0],axis='columns')
# X=d.drop([1],axis='columns')
# d.rename(index=str,columns={"1":"Happy"})
# Y=d[(1)]
# # X=X.drop(['Circuits'],axis='columns')
d

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(X_train, Y_train)
Y_pred=model.predict(X_test)
model.score(X_test, Y_test)

from sklearn.metrics import confusion_matrix
print(confusion_matrix( Y_test ,Y_pred ))

X

X=X.drop(['Circuit'],axis='columns')

from itertools import combinations
X=X.to_numpy()
import numpy as np
# X=np.transpose(X)

##Creating 70 unique combinations, having 4 features each
import numpy as np
X=np.transpose(X)
a=list(combinations(X, 4))
b=[]
for i in range(70):
  b.append(pd.DataFrame(np.transpose(a[i])))

##Traing 70 datasets and calculating score and (score/RMSE+0.001) for each Decision tree
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import numpy as np
import random
import math
ar=[]
are=[]
arp=[]
r=0
arratio=[]


while r<=69:

    P_train, P_rem, Q_train, Q_rem = train_test_split(b[r],Y,test_size=0.2)
    P_valid, P_test, Q_valid, Q_test = train_test_split(P_rem,Q_rem, test_size=0.5)
    classifier=DecisionTreeClassifier()
    classifier = classifier.fit(P_train,Q_train)

    res=classifier.predict(P_valid)

    s=classifier.score(P_valid,res)
    resCount=0
    Q_validCount=0
    Q_valid=Q_valid.to_numpy()
    for i in range(len(P_valid)):
      if(res[i]==True):
        resCount+=1
      if(Q_valid[i]==True):
        Q_validCount+=1

    ar.append(s)
    MSE = np.square(np.subtract(resCount,Q_validCount)).mean()

    RMSE = math.sqrt(MSE)
    are.append(RMSE)
    arp.append(b[r])
    if not RMSE==0:
      arratio.append(((s/(RMSE))*10))
    else:
      arratio.append((s*10))


    print(RMSE,s,arratio)












    r=r+1

##Calculating accuracy for all the random forests,having decision trees between 0.5 interval of arratio
S=[]
count=[]
th=[]

for j in range(6,11):

    ar2=[]
    for k in range(70):
        if j in arratio and (j==arratio[k]):
            th.append((j+j+0.5)/2)
            P=arp[k]

            P_train, P_valid, Q_train, Q_valid = train_test_split(P,Y,test_size=0.2)
            classifier=DecisionTreeClassifier()
            classifier = classifier.fit(P_train,Q_train)
            res=classifier.predict(P_valid)
            ar2.append(res)
    arr=[]
    arr.append(0)
    arr.append(0)
    ar3=[]

    for m in range(len(res)):
        arr[0]=0
        arr[1]=(0)
        for n in range(len(ar2)):
            if ar2[n][m]==0:
                arr[0]=arr[0]+1
            else:
                arr[1]=arr[1]+1

        if arr[0]>=arr[1]:
            ar3.append(0)
        else:
            ar3.append(1)

    count1=0
    S=[]
    S= Q_valid.to_numpy()
    for i in range(len(res)):
        if ar3[i]==S[i]:
            count1=count1+1
    count1=count1/len(res)

    count.append(count1)
    print(count,th)

#Creating final random forests with all decision trees having maximum accuracy
import numpy as np
thh=[]
C=max(count)
ar4=[]
acc=[]

for i in range(len(count)):
    if C==count[i]:
        j=i
        t=th[j]
        thh.append(t)
for d in range(len(thh)):
    for i in range(len(arp)):
        t=thh[d]
        if arratio[i]>=(t-0.5) and arratio[i]<=(t+0.5):
            P=arp[i]
            P_train, P_rem, Q_train, Q_rem = train_test_split(P,Y,test_size=0.2)

            classifier=DecisionTreeClassifier()
            classifier = classifier.fit(P_train,Q_train)
            res=classifier.predict(P_rem)
            ar4.append(res)
            acc.append(classifier.score(P_rem,Q_rem))
arrf=[]
arrf.append(0)
arrf.append(0)
ar5=[]
one=0
zer=0
for m in range(len(res)):
    arrf[0]=0
    arrf[1]=(0)

    for n in range(len(ar4)):

            if ar4[n][m]==0:
                arrf[0]=arrf[0]+1*(acc[n])
                arrf[1]=arrf[1]+(1-acc[n])



            else:
                arrf[1]=arrf[1]+(acc[n])*1
                arrf[0]=arrf[0]+(1-acc[n])


    if arrf[0]>=(0.6)*(arrf[1]):
        ar5.append(0)
    else:
        ar5.append(1)


Q_rem=Q_rem.to_numpy()
count1=0
for i in range(len(res)):
    if ar5[i]==Q_rem[i]:
        count1=count1+1
count1=count1/len(res)
tp,tn,fp,fn=0,0,0,0
z=0
on=0
for i in range(len(ar5)):
    if Q_rem[i]==0:
        z=z+1
    else:
        on=on+1


count1

##Creating classification report
tp,tn,fp,fn=0,0,0,0
for i in range(len(ar5)):
    if ar5[i]==1:
        if Q_rem[i]==1:
            tp=tp+1
        else:
            fp=fp+1
    elif ar5[i]==0:
        if Q_rem[i]==0:
            tn=tn+1
        else:
            fp=fp+1
print(tp,fp,tn,fn)

tp

fp

tn

fn

##F-score calculation
pre=(tp/(tp+fp))
re=(tp/tp+fn)
score=((2*pre*re)/(pre+re))
score